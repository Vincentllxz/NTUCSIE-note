\lecture{3}{16 Sep. 13:20}{}
\section{Triangular Factors and Row Exchanges}

\[
\boxed{\red{A}x = b}
\]
\[
\Longrightarrow \quad \red{LU}x = b \quad \Longrightarrow \quad \begin{cases}
    Lc = b \\
    Ux = c
\end{cases}
\]

\begin{eg}
\[
Ax = \begin{pmatrix}
    2 & 4 & -2 \\
    4 & 9 & -3 \\
    -2 & -3 & 7
\end{pmatrix} \begin{pmatrix}
    u \\ v \\ w
\end{pmatrix} = \begin{pmatrix}
    2 \\ 8 \\ 10
\end{pmatrix} = b
\]
\end{eg}

\begin{remark}
    \(\ell\): multipliers
    \[
        E_{ij}(\ell): (\text{i-th row}) + (-\ell)(\text{j-th column})
    \]
\end{remark}

\[
    \begin{pmatrix}
        2 & 4 & -2 & 2 \\
        4 & 9 & -3 & 8 \\
        -2 & -3 & 7 & 10
    \end{pmatrix} \quad \xrightarrow[\blue{R_3 + (1)R_1}]{\blue{R_2 + (-2)R_1}} \quad \begin{pmatrix}
        2 & 4 & -2 & 2 \\
        0_{\blue{21}} & 1 & 1 & 4 \\
        0 & 1 & 5 & 12
    \end{pmatrix} \quad \xrightarrow{\blue{R_3 + (-1)R_2}} \quad \begin{pmatrix}
        \boxed{\red{2}} & 4 & -2 & 2 \\
        0 & \boxed{\red{1}} & 1 & 4 \\
        0 & 0_{\blue{32}} & \boxed{\red{4}} & 8
    \end{pmatrix} \quad \text{\red{pivot}}
\]

\[
E_{21}\red{(2)} = E = \begin{pmatrix}
    1 & 0 & 0 \\
    \blue{-2} & 1 & 0 \\
    0 & 0 & 1
\end{pmatrix}, \quad E_{31}\red{(-1)} = F = \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    \blue{1} & 0 & 1
\end{pmatrix}, \quad E_{32}\red{(1)} = G = \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & \blue{-1} & 1
\end{pmatrix}
\]
i.e. \[
E_{21}E_{31}E_{32}Ax = \begin{pmatrix}
    2 & 4 & -2 \\
    0 & 1 & 1 \\
    0 & 0 & 4
\end{pmatrix} \begin{pmatrix}
    u \\ v \\ w
\end{pmatrix} = Ux = c = \begin{pmatrix}
    2 \\ 4 \\ 8
\end{pmatrix} = E_{21}E_{31}E_{32}b
\]

\begin{exercise}
How can we undo the steps of Gaussian Elimination?
\end{exercise}

\[
\red{E^{-1}F^{-1}G^{-1}}GFEA = A = \underbrace{E^{-1}F^{-1}G^{-1}}\boxed{U} = LU \quad \text{i.e.} \quad A=\underset{\red{\text{factors of }A}}{LU}
\]

\[
E^{\red{-1}} = \begin{pmatrix}
    1 & 0 & 0 \\
    \red{-}(\blue{-2}) & 1 & 0 \\
    0 & 0 & 1
\end{pmatrix}, \quad F^{\red{-1}} = \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    \red{-}(\blue{1}) & 0 & 1
\end{pmatrix}, \quad G^{\red{-1}} = \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & \red{-}(\blue{-1}) & 1
\end{pmatrix}
\]

\[
E^{-1}F^{-1}G^{-1} = \begin{pmatrix}
    1 & 0 & 0 \\
    \yelbox{2} & 1 & 0 \\
    \yelbox{-1} & \yelbox{1} & 1
\end{pmatrix} \yel{\Longrightarrow \text{records everything that has been done so far}}
\]

\newpage
\subsection{Triangular Factorization}

\begin{theorem}
    If no exchanges are required, the original matrix $A$ can be written as \[
    A = LU
    \]
    \begin{itemize}
        \item The matrix $L$ is lower triangular with 1's on the diagonal and the multipliers $\ell_{ij}$ (taken from elimiation) below the diagonal. 
        \item The matrix $U$ is the upper triangular matrix which appears after forward elimination and before back-substitution; its diagonal entries are the pivots.
    \end{itemize}
\end{theorem}

\begin{eg}
\begin{align*}
    \begin{pmatrix}
        2 & -1 & -1 \\
        0 & -4 & 2 \\
        6 & -3 & 0
    \end{pmatrix} &= \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        3 & 0 & 1
    \end{pmatrix} \begin{pmatrix}
        2 & -1 & -1 \\
        0 & -4 & 2 \\
        0 & 0 & 4
    \end{pmatrix} \begin{matrix}
        \Rightarrow \; \blue{\text{提出}2} \\ \\ \\
    \end{matrix}
     \\ \\
    &= \begin{pmatrix}
        \blue{2} & 0 & 0 \\
        \blue{0} & -4 & 0 \\
        \blue{6} & 0 & 3
    \end{pmatrix} \begin{pmatrix}
        \blue{1} & \blue{-1/2} & \blue{-1/2} \\
        0 & 1 & -1/2 \\
        0 & 0 & 1
    \end{pmatrix}
\end{align*}
\end{eg}

\begin{exercise}
\[
A = \begin{pmatrix}
    4 & 5 \\
    1 & 2
\end{pmatrix} \quad \text{;} \quad A = \begin{pmatrix}
    2 & 6 & 5 \\
    -1 & 4 & -2 \\
    1 & 2 & 3
\end{pmatrix} \quad \text{;} \quad \underset{\red{\text{"triangular matrix" 有三條對角線}}}{A = \begin{pmatrix}
    1 & -1 & 0 & 0 \\
    -1 & 2 & -1 & 0 \\
    0 & -1 & 2 & -1 \\
    0 & 0 & -1 & 2
\end{pmatrix}}
\]
\end{exercise}
\begin{answer}

\end{answer}

\newpage

\subsection{One Linear System = Two Triangular Systems}
\[
Ax = b \quad \Longrightarrow \quad Ux = c \text{ \& } Lc = b \quad \Longrightarrow \quad A=LU
\]
\begin{remark}
    The \(LU\) form is unsysttematic in one aspect. $U$ has pivots along its diagonal where $L$ always has 1's.

    We can rewrite $U$ as \[
    U = \begin{pmatrix}
        u_{11} & u_{12} & \cdots & u_{1n} \\
        0 & u_{22} & & u_{2n} \\
        \vdots & & \ddots & \vdots \\
        0 & 0 & \cdots & u_{nn}
    \end{pmatrix} = \begin{pmatrix}
        \red{u_{11}} & 0 & \cdots & 0 \\
        0 & \blue{u_{22}} & & 0 \\
        \vdots & & \ddots & \vdots \\
        0 & 0 & \cdots & u_{nn}
    \end{pmatrix} \begin{pmatrix}
        1 & \red{u_{12}/u_{11}} & \cdots & \red{u_{1n}/u_{11}} \\
        0 & 1 & & \blue{u_{2n}/u_{22}} \\
        \vdots & & \ddots & \vdots \\
        0 & 0 & \cdots & 1
    \end{pmatrix}
    \]
\end{remark}

\begin{eg}
\begin{align*}
    A &= \begin{pmatrix}
        3 & 4 \\
        1 & 2
    \end{pmatrix}\\ \\ &= \begin{pmatrix}
        1 & 0 \\
        1/3 & 1
    \end{pmatrix} \begin{pmatrix}
        \redbox{3} & 4 \\
        0 & \redbox{2/3}
    \end{pmatrix} = \underset{\red{L}}{\begin{pmatrix}
        1 & 0 \\
        1/3 & 1
    \end{pmatrix}} \underset{\red{D}}{\begin{pmatrix}
        3 & 0 \\
        0 & 2/3 
    \end{pmatrix}} \underset{\red{U}}{\begin{pmatrix}
        1 & 4/3 \\
        0 & 1
    \end{pmatrix}}
\end{align*}

\end{eg}

\begin{theorem}
    If \[ A = L_1D_1U_1 \text{ and } A = L_2D_2U_2\]
    then \[L_1 = L_2, D_1 = D_2, U_1 = U_2\]
    i.e. if $A$ has $LDU$ decomposition, then it \red{\textbf{is}} unique.
\end{theorem}

\newpage

\subsection{Row Excahnge and Permutation Matrices}
\[
P = \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 0 & 1 \\
    0 & 1 & 0 
\end{pmatrix}
\quad \red{\langle \text{Permutation matrix } P_{ij} \rangle}
\]

\begin{note}
    Permutation matrix is also an elementary matrix.
\end{note}

\begin{eg}
    Here are some of the example:

\begin{enumerate}[label=$\arabic*^\circ$]
    \item \[
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 0 & 1 \\
        0 & 1 & 0 
    \end{pmatrix} \begin{pmatrix}
        1 \\ 3 \\5
    \end{pmatrix} = \begin{pmatrix}
        1 \\ 5 \\ 3
    \end{pmatrix}
    \quad \boxed{R_2\leftrightarrow R_3}
    \]

    \item \[
    PA = \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 0 & 1 \\
        0 & 1 & 0 
    \end{pmatrix} \begin{pmatrix}
        2 & 4 & 1 \\
        0 & 0 & 3 \\
        0 & 6 & 5
    \end{pmatrix} = \begin{pmatrix}
        2 & 4 & 1 \\
        0 & 6 & 5 \\
        0 & 0 & 3
    \end{pmatrix}
    \quad \boxed{R_2\leftrightarrow R_3}
    \] 
    
    \item \[
    AP = \begin{pmatrix}
        2 & 4 & 1 \\
        0 & 0 & 3 \\
        0 & 6 & 5
    \end{pmatrix} \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 0 & 1 \\
        0 & 1 & 0 
    \end{pmatrix} = \begin{pmatrix}
        2 & 1 & 4 \\
        0 & 3 & 0 \\
        0 & 5 & 6 
    \end{pmatrix}
    \quad \boxed{C_2\leftrightarrow C_3}
    \]
\end{enumerate}
\end{eg}

\begin{note}
    For the permutation matrix:
    \begin{enumerate}[label=$\arabic*^\circ$]
        \item $PA$: Performing row exchange of $A$
        \item $AP$: Performing column exchange of $A$ 
        \item $\blue{P}Ax = \blue{P}b$; Should we permute the component of $x = \begin{pmatrix}u\\v\\w\end{pmatrix}$ as well? \red{\textbf{NONONONONO!!!}}
    \end{enumerate}
\end{note}

\begin{eg}
    \[
    A = \begin{pmatrix}
        0 & a & b \\
        0 & 0 & c \\
        d & e & f
    \end{pmatrix}
    \qquad Ax = b 
    \]
\end{eg}
\begin{enumerate}[label=(\arabic*)]
    \item if $d = 0$, the problem is incurable. The matrix is singular.
    \item if $d \neq 0$, $P_{13}A = \begin{pmatrix}
        d & e & f \\
        0 & 0 & c \\
        0 & a & b 
    \end{pmatrix}$
    ; if $a \neq 0$, $P_{23}P_{13}A = \begin{pmatrix}
        d & e & f \\
        0 & a & b \\
        0 & 0 & c 
    \end{pmatrix}$
\end{enumerate}

\vspace{1em}

\begin{center}
    \begin{tabular}{|cccc|}
         & $P_{23}P_{13}$ & $\red{\neq}$ & $P_{13}P_{23}$ \\
         &&& \\
        row & $\begin{matrix}
            1 \\ 2 \\ 3
        \end{matrix} \rightarrow \begin{matrix}
            3 \\ 2 \\ 1
        \end{matrix} \rightarrow \begin{matrix}
            3 \\ 1 \\ 2
        \end{matrix}$ & & $\begin{matrix}
            1 \\ 2 \\ 3
        \end{matrix} \rightarrow \begin{matrix}
            1 \\ 3 \\ 2
        \end{matrix} \rightarrow \begin{matrix}
            2 \\ 3 \\ 1
        \end{matrix}$
    \end{tabular}
\end{center}

\newpage

\begin{theorem}
    We seperate into two cases:
    \begin{itemize}
        \item In the non singular case, there's a permutation matrix $P$ that reorders the rows of $A$ to avoid zeros in the pivot positions.
        In this case, 
        \begin{enumerate}[label=(\arabic*)]
            \item $Ax=b$ has \blue{a unique} solution.
            \item It is found by \blue{elimination with row exchange}
            \item With the rows reorders in advance, $PA$ can be factored into $\blue{LU} \ \langle PA=LU \rangle$
        \end{enumerate}
        \item In singular case, no reordering can produce a full set of pivots.
    \end{itemize}
\end{theorem}

\begin{eg}
    \[
    A = \begin{pmatrix}
        1 & 1 & 1 \\
        1 & 1 & 3 \\
        2 & 5 & 8
    \end{pmatrix} \xrightarrow[\red{\xout{\ell_{31} = 2}} \ \grn{\ell_{21} = 2}]{\red{\xout{\ell_{21} = 1}} \ \grn{\ell_{31} = 1}} \begin{pmatrix}
        1 & 1 & 1 \\
        0 & 0 & 2 \\
        0 & 3 & 6
    \end{pmatrix}\xrightarrow{P_{23}} \begin{pmatrix}
        1 & 1 & 1 \\
        0 & 3 & 6 \\
        0 & 0 & 2 
    \end{pmatrix} = U
    \]
    
    \[
    L = \red{\begin{pmatrix}
        1 & 0 & 0 \\
        1 & 1 & 0 \\
        2 & 0 & 1
    \end{pmatrix}} \ \red{\langle \text{This is WORNG} \rangle} \ = \grn{\begin{pmatrix}
        1 & 0 & 0 \\
        2 & 1 & 0 \\
        1 & 0 & 1
    \end{pmatrix}}
    \]
\end{eg}
\vspace{1em}
\textbf{To summarize}: A good code for Gaussian Elimination keeps a record of $L, U$ and $P$. They allow the solution ($Ax=b$) from two triangular systems.
If the system $Ax=b$ has a unique solution, they we say:
\begin{enumerate}[$\arabic*^\circ$]
    \item The system is nonsingular or
    \item The matrix is nonsingular
\end{enumerate}
Otherwise, it is singular.

\section{Inverse and Transpose}

\begin{definition}
    An $n\times n$ matrix $A$ is \red{invertible} if $\exists$ an $n\times n$ matrix $B \ni BA = I = AB$
\end{definition}

\begin{theorem}
    If $A$ is invertible, then the matrix $B$ satisfying $AB = BA = I$ is unique!
\end{theorem}
\begin{proof}
    Suppose $\exists c \neq B \ni AC = CA = I$
    \[
    B = BI = B(AC) = (BA)C = IC = C \ \text{i.e} \ B = C
    \]
    we call this matrix $B$, the \redbox{inverse of $A$}, and denoted as \redbox{$A^{-1}$}
\end{proof}

\begin{note}
Not all $n\times n$ matrices have inverse.

e.g. 
\begin{enumerate}[$\arabic*^\circ$]
    \item \[
    \begin{pmatrix}
        0 & 0 \\
        0 & 0
    \end{pmatrix} \ \begin{pmatrix}
        1 & 1 \\
        1 & 1
    \end{pmatrix}
    \]
   
    \item if $Ax = \vec{0}$ has a \uline{$\overset{\blue{x\neq 0}}{\text{nonzero solution}}$}, then $A$ has no inverse!
    \[
    x = A^{-1}(Ax) = A^{-1}\vec{0} = \vec{0} \quad (\rightarrow\leftarrow)
    \]
\end{enumerate}
\end{note}

\begin{note}
The inverse of $A^{-1}$ is $A$ itself. i.e. $(A^{-1})^{-1} = A$.
\end{note}

\begin{note}
If $A = (a)_{1\times 1}$ and $a \neq 0$, then $A^{-1} = (\frac{1}{a})$.
The inverse of \(\begin{pmatrix}
    a & b \\
    c & d
\end{pmatrix}_{2\times2}\) is $$\frac{1}{\det(A)} \begin{pmatrix}
    d & -b \\
    -c & a
\end{pmatrix} \text{ if } \det(A) \neq 0$$
\end{note}

\begin{note}
\[
A = \begin{pmatrix}
    d_1 & \cdots & 0 \\
     & \ddots & \\
    0 & \cdots & d_n
\end{pmatrix} \quad d_i \neq 0, \; \forall i \quad \Longrightarrow A^{-1} = \begin{pmatrix}
    1/d_1 & \cdots & 0 \\
     & \ddots & \\
    0 & \cdots & 1/d_n
\end{pmatrix}
\]
\end{note}

\begin{proposition}
    If $A$ and $B$ are invertible, then 
    \begin{itemize}
        \item $(AB)^{-1} = \blue{B^{-1}A^{-1}}$
        \item \(
        (A_1A_2\cdots A_n)^{-1} = A_n^{-1}\cdots A_2^{-1}A_1^{-1}
        \)
    \end{itemize}
\end{proposition}

\subsection{The Calculation of $A^{-1}$: Gaussian-Jordan Method}

\[
    A\cdot A^{-1} = I
\]
\[
    A_{n\times n} B_{n\times n} = I_n
\]
\begin{align*}    
    &\Longrightarrow \ A_{n\times n}(B_1|B_2|\cdots|B_n)_{n\times n} = (e_1|e_2|\cdots|e_n)_{n\times n} \\
    &\Longrightarrow \ (\red{AB_1}|\blue{AB_2}|\cdots|\yel{AB_n})_{n\times n} = (\red{e_1}|\blue{e_2}|\cdots|\yel{e_n})_{n\times n} \\
    &\Longrightarrow \ AB_1 =e_1; \ AB_2 =e_2;\ \cdots ;\ AB_n = e_n \quad \blue{\longrightarrow} \quad \blue{n\text{ linear systems:} Ax=b}
\end{align*}

\begin{definition}[Gaussian-Jordan Method]
    Instead of stopping at $U$ and switching to back substitution, it continues by subtracting multipliers of a row from the rows above till it reaches a diagonal matrix. Then we divide each row by corresponding pivot.

    \[
    (\underset{\red{LU}}{A}|I) \xrightarrow{\red{\times L^-1}} (U|\red{L^{-1}}) \xrightarrow{\red{\times U^{-1}}} (I|\red{A^{-1}})
    \]
\end{definition}

\[
\left(
\begin{array}{ccc|ccc}
2 & -1 & 0 & 1 & 0 & 0 \\
-1 & 2 & -1 & 0 & 1 & 0 \\
0 & -1 & 2 & 0 & 0 & 1
\end{array}
\right) \;\longrightarrow\; \left(
\begin{array}{ccc|ccc}
\redbox{2} & -1 & 0 & 1 & 0 & 0 \\
0 & \redbox{3/2} & -1 & 1/2 & 1 & 0 \\
0 & 0 & \redbox{4/3} & 1/3 & 2/3 & 1
\end{array}
\right)
\]
\vspace{1em}
\[
\longrightarrow \; \left(
\begin{array}{ccc|ccc}
\redbox{2} & -1 & 0 & 1 & 0 & 0 \\
0 & \redbox{3/2} & -1 & 1/2 & 1 & 0 \\
0 & 0 & \redbox{4/3} & 1/3 & 2/3 & 1
\end{array}
\right)
\;\longrightarrow\;
\left(
\begin{array}{ccc|ccc}
1 & 0 & 0 & 3/4 & 1/2 & 1/4 \\
0 & 1 & 0 & 1/2 & 1 & 1/2 \\
0 & 0 & 1 & 1/4 & 1/2 & 3/4
\end{array}
\right)
\]
\vspace{1em}
\[
A^{-1} = \begin{pmatrix}
\sfrac{3}{4} & \sfrac{1}{2} & \sfrac{1}{4} \\
\sfrac{1}{2} & 1 & \sfrac{1}{2} \\
\sfrac{1}{4} & \sfrac{1}{2} & \sfrac{3}{4}
\end{pmatrix}
\]

\newpage

\subsection{Invertible = Nonsingular}

\begin{exercise}
What kind of matrices are invertible?
\end{exercise}
\begin{answer}
Here are the example:
\begin{enumerate}[$\arabic*^\circ$]
    \item nonzero pivot \blue{Ch1} \blue{Ch4}
    \item nonzero determinants \blue{Ch4}
    \item independent columns (rows) \blue{Ch2}
    \item nonzero eigenvalues \blue{Ch5}
\end{enumerate}
which will in the whole course 
\end{answer}

\vspace{2em}

Suppose a matrix $A$ has full set of nonzero pivots. By definition, $A$ is nonsingular and the $n$ systems \[
    Ax_1 = e_1, \ Ax_2 = e_2,\ \cdots\ , Ax_n = e_n
\]
can be solved by elimination or Gaussian-Jordan Method.
\vspace{1em}

Row exchanges maybe necessary, but the columns of $A^{-1}$ are uniquely determined.
\begin{align*}
    Ax = b \quad &PAx = Pb \\
                &PAx_i = Pe_i
\end{align*}
\[
\{ Pe_1, Pe_2, \cdots, Pe_n\} = \{ e_1, e_2, \cdots, e_n\} 
\]

\begin{note}
    \textbf{Compute} $A^{-1}$: 
    \begin{enumerate}[$\arabic*^\circ$]
        \item $A(x_1|\cdots|x_n) = I = (e_1|\cdots|e_n) \ \Longleftrightarrow \ Ax_i = e_i, \; i=i\cdots n$
        \item Gauss-Jordan Method: $(\ A\ |\ I\ ) \ \longrightarrow \ (\ I\ |\ A^{-1}\ )$
    \end{enumerate}
\end{note}

\begin{exercise}
We have found a matrix $A^{-1} \ni AA^{-1} = I$. But is $A^{-1}A = I$
\end{exercise}
\begin{answer}
    We can do this by recall.
    \begin{prev}
        Recall that every Gauss-Jordan step is a multiplication of matrices on the left. There are three types of elementary matrices:
    \end{prev}
    \begin{enumerate}[$\arabic*^\circ$]
        \item $\boxed{E_{ij}(\ell)}$ : to subtract a multiple $\ell$ of $j$ row from $i$ row.
        \item $\boxed{P_{ij}}$ : to exchange row $i$ and $j$
        \item $\boxed{D_i(d)}$ : to multiply row $i$ by $d$ i.e. $D_i(d) = \begin{pmatrix}
            1 & & & & 0 \\
            0 & \ddots & & & \vdots\\
            \vdots & & d & & \vdots \\
            \vdots &  & & \ddots & 0\\
            0 & & & &  1 \\
        \end{pmatrix} \begin{matrix}
            \\ \\ \red{\rightarrow \text{ith row}} \\ \\ \\
        \end{matrix}$
        \[
            \begin{pmatrix}
                d_1 & & & 0\\
                 & 1 & & \\
                 & & \ddots & \\
                0 & & & 1
            \end{pmatrix} \begin{pmatrix}
                1 & & & 0\\
                 & d_2 & & \\
                 & & \ddots & \\
                0 & & & 1
            \end{pmatrix} \cdots \begin{pmatrix}
                1 & & & 0\\
                 & 1 & & \\
                 & & \ddots & \\
                0 & & & d_n
            \end{pmatrix} = \begin{pmatrix}
                d_1 & & & 0\\
                 & d_2 & & \\
                 & & \ddots & \\
                0 & & & d_n
            \end{pmatrix}
        \]
        \[
        \Longrightarrow \quad \blue{DEEPEE}A = I \quad \Longrightarrow \quad \blue{A^{-1}} A = I \therefore \text{we have a left inverse!} 
        \]
    \end{enumerate}

    These are the operation of $A^{-1}$
\end{answer}

\begin{theorem}
    For nonsingular and invertible:
    \begin{itemize}
        \item Every nonsingular matrix is invertible.
        \item Every invertible matrix is nonsingular.
    \end{itemize}
\end{theorem}

\begin{theorem}
    A square matrix is invertible $\Longleftrightarrow$ it is nonsingular
\end{theorem}