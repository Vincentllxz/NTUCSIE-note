\lecture{6}{14 Oct. 13:20}{}

\begin{remark}[1]
    To show that $v_1, \cdots , v_n$ are linearly independent. We vaerify if 
    \[
        c_1 v_1 + c_2 v_2 + \cdots + c_n v_n = 0 \text{ for some } c_i \in F
    \]
    then $c_i$ \blue{must be} zero for all $i$.
\end{remark}

\begin{eg}
In $\mathbb{R}^2$, if $v_1, v_2$ are not colinear(共線) then they are linearly \blue{independent}. 
\[
v_1(\neq 0) \text{ and } v_2(\neq 0) \text{ are linearly dependent } \Longleftrightarrow v_1, v_2 \text{ are on the same } \blue{line}
\]


Any 3 vectors in $\mathbb{R}^2$ are linearly dependent.
\end{eg}

\begin{remark}[2]
    If $v_1 = v_2$, then the set $\{v_1, \cdots v_n\}$ is linearly \blue{dependent}.

    \[
        \alpha v_1 + (-\alpha) v_2 = 0
    \]
\end{remark}

\begin{remark}[3]
    Any set which contain a linear dependent subset is linearly \blue{dependent}.
\end{remark}

\begin{remark}[4]
    Any subset of a linearly independent set is linearly \blue{independent}.
\end{remark}

\begin{remark}[5]
    Any set which contain 0 vector is linearly \blue{dependent}.
\end{remark}

\begin{eg}
    \[
        A = \begin{pmatrix}
            1 & 2 & 1 & -1 \\
            3 & 2 & -3 & 0 \\
            -4 & -4 & 2 & 1 \\
            \underset{v_1}{-2} & \underset{v_2}{0} & \underset{v_3}{-4} & \underset{v_4}{0}
        \end{pmatrix}
    \]
    The columns of $A$ are linearly \red{dependent}.
\end{eg}

\[
    c_1 v_1 + c_2 v_2 + c_3 v_3 + c_4 v_4 = 0
\]
\[
    (v_1\ v_2\ v_3\ v_4) \begin{pmatrix}
        c_1 \\ c_2 \\ c_3 \\ c_4
    \end{pmatrix} = 0 \quad \Longrightarrow 4v_1 + (-3)v_2 + 2v_3 + 0v_4 = 0
\]

\newpage
\begin{eg}
    \[
        A = \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            -1 & -1 & -1 & 1
        \end{pmatrix}
    \]
    The columns of $A$ are linearly \red{independent}
\end{eg}

\begin{note}
    We showed that the nullspace of $A$ is $\{0\}$ only. That is exactly the same as saying the columns of $A$ are linearly independent.
\end{note}

\begin{eg}
    \[
    U = \begin{pmatrix}
        \redbox{1} & 3 & 3 & 2 \\
        0 & 0 & \redbox{3} & 1 \\
        0 & 0 & 0 & 0 \\
    \end{pmatrix}
    \]
\end{eg}

\begin{proposition}[2F]
    The $r$ nonzero rows of echelon matrix $U$ are linearly independent, and so are $r$ columns that contain pivots.
    \begin{eg}
        In $\mathbb{R}^n$, $e_1, e_2, \cdots, e_n$ are linearly \red{independent}.
    \end{eg}
\end{proposition}

\vspace{2em}
\textbf{To summarize}: To check any set of vectors $v_1, v_2, \cdots, v_n(\in \mathbb{R}^n)$ are linearly independent. \\
Let $A = (v_1|v_2|\cdots|v_n)_{\red{m \times n}}$, then solve $Ax_{\red{n \times 1}} = 0$.
\begin{enumerate}[$\arabic*^\circ$]
    \item if $\exists$ solution $x \neq 0$, then $v_i$'s are linearly \blue{dependent}.
    \item if there are no free variables (i.e. $\rank(A) = n$), \blue{nullspace $= \{0\}$} then $v_i$'s are linearly \blue{independent}.
    \item if $\rank(A) < n$, then $v_i$'s are linearly \blue{dependent}.
    \item special case: if $v_i \in \mathbb{R^m}$ and $n > m$, then $v_i$'s are linearly \blue{dependent}.
\end{enumerate}

\begin{proposition}
    A set of $n$ vectors in $\mathbb{R}^m$ must be linearly dependent if \redbox{$n > m$}.
\end{proposition}

\newpage

\subsection{Spanning a Subspace}
\begin{definition}[2H]
    Let $S$ be a subset vectors in $V/F$. \\
    \redbox{The subspace spanned by $S$} is defined to be the intersection $W$ of all subspaces of $V$ which contain $S$. \\
    When $S$ is finite, $S = \{v_1, \cdots, v_n\}$, we call $W$ the subspace spanned by $v_1, \cdots, v_n$ and denoted as $W = \red{\langle v_1, \cdots, v_n \rangle}$ or $W = \text{span}(S) = \red{\langle S \rangle}$.
\end{definition}

\begin{theorem}
    \red{[}The subspace spanned by a nonempty subset $S$\red{]} of a vector space $V$ is \red{[}the set $T$ of all linear combinations of vectors in $S$\red{]}.
\end{theorem}
\begin{proof}
    We need to show $W = T$.
    \begin{claim}
        $W = T$ if and only if $W \subseteq T$ and $T \subseteq W$.
    \end{claim}
    \begin{itemize}
        \item Let $W$ be the subspace spanned by $S$, $S \subseteq W$ ($S$ 不一定有包含 0 vector 所以不能用 $\leq$). \\
        So $\underset{\grn{T}}{\grn{\text{every linear combination of vectors in }S}}$ is in $W$. \grn{$\Longrightarrow T \subseteq W$}. \\
        ($\because W$ is a subspace which is a vector space)
        \item 
        on the other hand, $T$ is a subspace containing $S$. \\
        ($\because x, y \in T, \ \alpha \in F \ \Rightarrow \ \alpha x + y \in T$)
    \end{itemize}
    So, $W \subseteq T$ by definition $\Rightarrow W = T$. \\
    (Intersection of all subspaces containin $S$)
\end{proof}

\begin{eg}
    $\mathcal{C}(A) = $ space spanned by columns of $A$.
\end{eg}

\begin{eg}
    $w_1 = (1, 0, 0), \ w_2 = (0, 1, 0), \ w_3 = (0, 0, 1),$ span a \blue{space $\mathbb{R}^3$}. \\
    $w_1 = (1, 0, 0), \ w_2 = (0, 1, 0), \ w_3 = (-3, 0, 0),$ span a \blue{plane $\mathbb{R}^2$}.
\end{eg}

\begin{note}
    Spanning involves the \blue{columns space}, independence involves the \blue{null space}.
\end{note}

\subsection{Basis}

\begin{definition}[2I]
    A \redbox{basis} for a vector space is a set of vectors that satisfies
    \begin{enumerate}[label=(\roman*)]
        \item it is linearly independent \red{AND}
        \item it span the vector space
    \end{enumerate}
    If the basis of $V$ is finite, then $V$ is finite-dimensional (f-dim).
\end{definition}

\begin{remark}[1]
    There's one and \blue{only one} way to write every $v \in V$ as a linear combination of the basis elements.
\end{remark}

\newpage

\begin{remark}[2]
    In $\mathbb{R}^n$, 
    \[
    e_i = \begin{pmatrix}
        0 \\ \vdots \\ 1 \\ \vdots \\ 0
    \end{pmatrix}_{n\times 1}\begin{matrix}
        \uparrow \\ i^{th} \\ \downarrow
    \end{matrix}
    \]
    then $\{e_1, \vdots, e_n\}$ is a basis for $\mathbb{R}^n$. The basis is called the \red{standard basis}.
    \[
        \forall x = (x_1, \cdots, x_n) \in \mathbb{R}^n, \ x = \sum_{i=1}^n x_i e_i
    \]
    The standard basis \blue{is not} the only basis for $\mathbb{R}^n$. In fact, there are \blue{infinitely many} bases for $\mathbb{R}^n$. For any nonsingular matrix $A_{n \times n}$, the \red{columns} of $A$ are the basis for $\mathbb{R}^n$.
\end{remark}

\begin{eg}
    \[
    A = \begin{pmatrix}
        1 & 3 & 3 & 2 \\
        2 & 6 & 9 & 5 \\
        -1 & -3 & 3 & 0 \\
    \end{pmatrix}_{3 \times 4} \quad \longrightarrow \quad U = \begin{pmatrix}
        \redbox{1} & 3 & 3 & 2 \\
        0 & 0 & \redbox{3} & 1 \\
        0 & 0 & 0 & 0 \\
    \end{pmatrix}_{3 \times 4}
    \]
    The columns of $U$ that contain pivots are a basis for $\mathcal{C}(U)$. \\
    Note that $\mathcal{C}(U)$ is generate by $\begin{pmatrix}
        1 \\ 0 \\ 0
    \end{pmatrix}$ and $\begin{pmatrix}
        1 \\ 1 \\ 0
    \end{pmatrix}$, which is a $xy$-plane within $\mathbb{R}^3$.
    
    
\end{eg}

\begin{remark}
    $\mathcal{C}(U)$ is \red{NOT} same as $\mathcal{C}(A)$.
\end{remark}

\begin{theorem}[2J]
Any two bases for $V$ contain the same number of vectors. This number is called the \emph{dimension} of $V$.
\end{theorem}

\begin{proof}
Suppose $v_1, \dots, v_m$ and $w_1, \dots, w_n$ are bases for $V$, and suppose $m < n$.

For $j = 1, \dots, n$,
\[
w_j = a_{1j} v_1 + \cdots + a_{mj} v_m \quad \text{for some } a_{ij} \in F.
\]

Let
\[
w = [w_1, \dots, w_n] = V A = [v_1, \dots, v_m]
\begin{bmatrix}
a_{11} & \cdots & a_{1n} \\
a_{21} & \cdots & a_{2n} \\
\vdots & \ddots & \vdots \\
a_{m1} & \cdots & a_{mn}
\end{bmatrix}.
\]

The matrix $A$ is $m \times n$ with $m < n$.  
By Theorem 2C, $\exists$ nontrivial $C$ such that $A C = 0$.

\[
V A C = W C = 0.
\]

Hence the columns of $W$ are linearly dependent.  
But the columns of $W$ are basis elements, contradiction $\;\Rightarrow\;$ $m \not< n$.

Similarly, we can show that $n \not< m$,  
so we conclude $m = n$.
\end{proof}

\newpage

\begin{theorem}[2L]
Any linearly independent set in a finite-dimensional vector space $V$ can be extended to a basis.  
Any spanning set of $V$ can be reduced to a basis.
\end{theorem}

\begin{proof}
Let $v_1, \dots, v_k$ be linearly independent over $F$.  
Then $\langle v_1, \dots, v_k \rangle \le V$.

If $\langle v_1, \dots, v_k \rangle = V$, then $\langle v_1, \dots, v_k \rangle$ is a basis of $V$.  
Otherwise, $\exists\, x \in V$ such that $x \notin \langle v_1, \dots, v_k \rangle$.

Then $x, v_1, \dots, v_k$ are linearly independent.  
If not, $\exists\, c \neq 0$, and $\exists\, \alpha_1, \dots, \alpha_k$, not all zero, such that
\[
c x + \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_k v_k = 0.
\]
\[
\Rightarrow\ 
x = c^{-1}\alpha_1 v_1 + c^{-1}\alpha_2 v_2 + \cdots + c^{-1}\alpha_k v_k.
\]
\[
\Rightarrow\ x \in \langle v_1, \dots, v_k \rangle, \;\text{contradiction.}
\]

Then repeat the process, i.e. is $\langle x, v_1, \dots, v_k \rangle = V$?  
Since $V$ is finite-dimensional, the process will terminate after finite steps.  

The 2nd half of the theorem can be proved similarly (exercise).
\end{proof}

\section{The Four Fundamental Subspaces}

Usually there are two ways to describe a subspace
\begin{enumerate}[label=(\roman*)]
    \item a set of vectors that span the space. \\
    (e.g. the column space of $A_{m \times n}$, $\mathcal{C}(A)$)
    \item a list of constraints that imposed on a subspace. \\
    (e.g. the null space of $A_{m \times n}$, $\mathcal{N}(A) = \{x \mid Ax = 0\}$)
\end{enumerate}
Here we will discuss four fundamental subspaces associated to $A_{m \times n}$
\begin{enumerate}[label=(\arabic*)]
    \item the \redbox{column space} of $A$ denoted by $\mathcal{C}(A)$
    \item the \redbox{null space} of $A$ denoted by $\mathcal{N}(A)$
    \item the \redbox{row space} of $A$ the columns spaces of $A^T$, denoted by $\mathcal{C}(A^T)$
    \item the \redbox{left null space} of $A$ denoted by $\mathcal{N}(A^T)$, i.e. $\{y \mid A^T y = 0\}$
    \begin{itemize}
        \item If $A_{m\times n}$, then $\mathcal{C}(A), \mathcal{N}(A^T) \leq \mathbb{R}^m$ and $\mathcal{N}(A), \mathcal{C}(A^T) \leq \mathbb{R}^n$.
    \end{itemize}
\end{enumerate}

\subsection{Row space $\mathcal{C}(A^T)$}

The \redbox{row space} of $A$ (the subspace spanned by the rows of $A$), $\mathcal{C}(A^T)$. For an echelon matrix, its $r$ nonzero rows are independent and its row space is \red{$r$-dimensional}.

\begin{proposition}[2M]
    The row space of $A$ has the same dimension $r$ as the row space of echelon form $U$ of $A$, and they have the same basis.
    \[
        \mathcal{C}(A^T) = \mathcal{C}(U^T)
    \]
    But in general, $\mathcal{C}(A) \neq \mathcal{C}(U)$.
\end{proposition}

